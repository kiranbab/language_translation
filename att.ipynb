{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math \n",
    "\n",
    "L=4 \n",
    "d_k=8 \n",
    "d_v=8 \n",
    "\n",
    "q =  np.random.randn(L,d_k)\n",
    "k = np.random.randn(L,d_k)\n",
    "v = np.random.randn(L,d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[-1.43357575 -0.43065121  0.77136426  0.19574309 -0.22401895 -1.04299148\n",
      "   1.10825192  0.20315761]\n",
      " [-0.34115764 -1.5118362   0.01821113 -1.5611847  -0.76708133  2.09346769\n",
      "  -0.67368897 -1.39134768]\n",
      " [ 0.53909654  0.6232494   1.74257407  1.2956597   0.18971572 -1.70160811\n",
      "  -1.06915727 -1.02722077]\n",
      " [-0.74246299 -1.87555485  0.99599366  0.55042828 -0.56317419 -0.71420677\n",
      "  -1.07514025 -1.38289993]]\n",
      "K\n",
      " [[-1.36513690e+00 -3.63309409e-01 -5.99209478e-01 -9.82497463e-01\n",
      "  -1.68603158e+00  4.11995122e-02  1.12483738e-01 -7.87179268e-02]\n",
      " [-1.16276551e+00  1.76990511e-01 -4.58214292e-01  1.15299911e-01\n",
      "   6.41515596e-01 -7.75446731e-01 -1.57954247e+00  2.88702225e-03]\n",
      " [-1.02698577e-01 -9.33278768e-01 -1.56137306e-01  9.57039865e-01\n",
      "   1.17119421e+00 -1.23056533e+00  1.81016119e-01  5.23565423e-01]\n",
      " [ 8.31014697e-01  8.22981509e-04  1.52106971e-01 -4.96104150e-02\n",
      "   2.78096315e-01  7.01630108e-01 -1.04268387e+00 -6.76145797e-01]]\n",
      "V\n",
      " [[ 0.78027041  0.49315609 -0.96264874  1.37189624 -0.04916138 -0.82434936\n",
      "   0.20130451  1.65966418]\n",
      " [-0.05033032  1.31207497 -0.03807837 -0.22575636  2.00510429 -1.01376449\n",
      "   0.60148031  0.51926522]\n",
      " [ 1.80522334  0.32469835 -0.96501699 -0.53727196  0.08058237  0.37153528\n",
      "   0.8252376   2.51931674]\n",
      " [-0.11591258 -0.88000712  1.20149022 -0.39754784 -0.50058359 -0.77109831\n",
      "   0.42053471  0.65937641]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\",q )\n",
    "print(\"K\\n\",k )\n",
    "print(\"V\\n\",v )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.90236139,  0.17493851,  1.94411624, -3.17107159],\n",
       "       [ 3.95125706, -1.11460719, -4.37591845,  2.69418582],\n",
       "       [-3.70889645,  1.96140921,  1.91566919,  1.31749333],\n",
       "       [ 1.46539482,  2.02522225,  1.4985709 ,  0.90399714]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q,k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0818524227722164, 0.5453739041230752, 5.472720986257096)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now divide by sqrt(d_k) since we want to reduce the variance \n",
    "q.var(), k.var(), np.matmul(q,k.T).var() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0818524227722164, 0.5453739041230752, 0.684090123282137)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q,k.T)/math.sqrt(d_k)\n",
    "q.var(), k.var(), scaled.var() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67258632,  0.0618501 ,  0.68734889, -1.12114311],\n",
       "       [ 1.39698033, -0.39407315, -1.5471208 ,  0.95253853],\n",
       "       [-1.31129291,  0.69346288,  0.67729134,  0.46580423],\n",
       "       [ 0.51809531,  0.71602419,  0.52982482,  0.31961125]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can see the reduction in variance of the producet \n",
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking \n",
    "* This is to ensure words dont get context from words generated in future\n",
    "* Not required in encoders(since all words are input at a time), but required in decoder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones((L,L)))\n",
    "mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask==0] = -np.infty \n",
    "mask[mask==1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.67258632,        -inf,        -inf,        -inf],\n",
       "       [ 1.39698033, -0.39407315,        -inf,        -inf],\n",
       "       [-1.31129291,  0.69346288,  0.67729134,        -inf],\n",
       "       [ 0.51809531,  0.71602419,  0.52982482,  0.31961125]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x),axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = softmax(scaled + mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.78027041,  0.49315609, -0.96264874,  1.37189624, -0.04916138,\n",
       "        -0.82434936,  0.20130451,  1.65966418],\n",
       "       [ 0.66154134,  0.61021532, -0.83048732,  1.143522  ,  0.24448277,\n",
       "        -0.85142504,  0.25850709,  1.49665143],\n",
       "       [ 0.86424383,  0.80144813, -0.52735298, -0.26886184,  0.98070499,\n",
       "        -0.35835217,  0.67995802,  1.52064345],\n",
       "       [ 0.60494068,  0.41952798, -0.24694202,  0.05607446,  0.51001227,\n",
       "        -0.57185013,  0.52195006,  1.32874961]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention,v)\n",
    "new_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.78027041,  0.49315609, -0.96264874,  1.37189624, -0.04916138,\n",
       "        -0.82434936,  0.20130451,  1.65966418],\n",
       "       [-0.05033032,  1.31207497, -0.03807837, -0.22575636,  2.00510429,\n",
       "        -1.01376449,  0.60148031,  0.51926522],\n",
       "       [ 1.80522334,  0.32469835, -0.96501699, -0.53727196,  0.08058237,\n",
       "         0.37153528,  0.8252376 ,  2.51931674],\n",
       "       [-0.11591258, -0.88000712,  1.20149022, -0.39754784, -0.50058359,\n",
       "        -0.77109831,  0.42053471,  0.65937641]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x),axis=-1)).T\n",
    "\n",
    "def scaled_dot_product_attention(q,k,v,mask=None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled = np.matmul(q,k.T)/math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled + mask \n",
    "    \n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention,v)\n",
    "    return out,attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " values, attention  = scaled_dot_product_attention(q,k,v,mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[-1.43357575 -0.43065121  0.77136426  0.19574309 -0.22401895 -1.04299148\n",
      "   1.10825192  0.20315761]\n",
      " [-0.34115764 -1.5118362   0.01821113 -1.5611847  -0.76708133  2.09346769\n",
      "  -0.67368897 -1.39134768]\n",
      " [ 0.53909654  0.6232494   1.74257407  1.2956597   0.18971572 -1.70160811\n",
      "  -1.06915727 -1.02722077]\n",
      " [-0.74246299 -1.87555485  0.99599366  0.55042828 -0.56317419 -0.71420677\n",
      "  -1.07514025 -1.38289993]]\n",
      "K\n",
      " [[-1.36513690e+00 -3.63309409e-01 -5.99209478e-01 -9.82497463e-01\n",
      "  -1.68603158e+00  4.11995122e-02  1.12483738e-01 -7.87179268e-02]\n",
      " [-1.16276551e+00  1.76990511e-01 -4.58214292e-01  1.15299911e-01\n",
      "   6.41515596e-01 -7.75446731e-01 -1.57954247e+00  2.88702225e-03]\n",
      " [-1.02698577e-01 -9.33278768e-01 -1.56137306e-01  9.57039865e-01\n",
      "   1.17119421e+00 -1.23056533e+00  1.81016119e-01  5.23565423e-01]\n",
      " [ 8.31014697e-01  8.22981509e-04  1.52106971e-01 -4.96104150e-02\n",
      "   2.78096315e-01  7.01630108e-01 -1.04268387e+00 -6.76145797e-01]]\n",
      "V\n",
      " [[ 0.78027041  0.49315609 -0.96264874  1.37189624 -0.04916138 -0.82434936\n",
      "   0.20130451  1.65966418]\n",
      " [-0.05033032  1.31207497 -0.03807837 -0.22575636  2.00510429 -1.01376449\n",
      "   0.60148031  0.51926522]\n",
      " [ 1.80522334  0.32469835 -0.96501699 -0.53727196  0.08058237  0.37153528\n",
      "   0.8252376   2.51931674]\n",
      " [-0.11591258 -0.88000712  1.20149022 -0.39754784 -0.50058359 -0.77109831\n",
      "   0.42053471  0.65937641]]\n",
      "New V \n",
      " [[ 0.78027041  0.49315609 -0.96264874  1.37189624 -0.04916138 -0.82434936\n",
      "   0.20130451  1.65966418]\n",
      " [ 0.66154134  0.61021532 -0.83048732  1.143522    0.24448277 -0.85142504\n",
      "   0.25850709  1.49665143]\n",
      " [ 0.86424383  0.80144813 -0.52735298 -0.26886184  0.98070499 -0.35835217\n",
      "   0.67995802  1.52064345]\n",
      " [ 0.60494068  0.41952798 -0.24694202  0.05607446  0.51001227 -0.57185013\n",
      "   0.52195006  1.32874961]]\n",
      "Attention \n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.85705639 0.14294361 0.         0.        ]\n",
      " [0.06357495 0.4719983  0.46442675 0.        ]\n",
      " [0.24687413 0.30090889 0.2497869  0.20243009]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\",q)\n",
    "print(\"K\\n\",k)\n",
    "print(\"V\\n\",v)\n",
    "print(\"New V \\n\", values )\n",
    "print(\"Attention \\n\", attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length =4 \n",
    "batch_size=1 \n",
    "input_dim = 512 \n",
    "d_model = 512 \n",
    "x = torch.randn((batch_size,sequence_length,input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_layer = nn.Linear(input_dim,3*d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv = qkv_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8 \n",
    "head_dim = d_model// num_heads\n",
    "qkv = qkv.reshape(batch_size,sequence_length,num_heads,3*head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
